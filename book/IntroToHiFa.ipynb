{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HistFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyhf` stands for **py**thon-based **H**ist**F**actory.\n",
    "\n",
    "It's a tool for statistical analysis of data in High Energy Physics.\n",
    "\n",
    "In this chapter, we will cover\n",
    "* What HistFactory is in general\n",
    "* What pyhf is specifically (and what it is not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis\n",
    "\n",
    "We divide analyses into the type of fit being performed:\n",
    "* unbinned analysis (based on individual observed events)\n",
    "* binned analyses (based on aggregation of events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img alt=\"WHgamgam unbinned distribution\" src=\"https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SUSY-2018-23/fig_04d.png\" width=300 style=\"display: inline\" />\n",
    "    <img alt=\"SUSY MBJ binned distribution\" src=\"https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/CONFNOTES/ATLAS-CONF-2018-041/fig_08a.png\" width=400 style=\"display: inline\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like HistFactory, `pyhf` does not work with unbinned analyses. These will not be covered in the tutorial.\n",
    "\n",
    "So what uses HistFactory?\n",
    "* [TRexFitter](https://trexfitter-docs.web.cern.ch/trexfitter-docs/) (ATLAS internal)\n",
    "* [WSMaker](https://gitlab.cern.ch/atlas-physics/higgs/hbb/WSMaker) (ATLAS internal)\n",
    "* [HistFitter](https://github.com/histfitter/histfitter)\n",
    "\n",
    "Most everyone in SUSY and Exotics who performs an asymptotic fit as part of their analysis is likely using HistFactory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Binned?\n",
    "\n",
    "Most likely, one performs a binned analysis if no functional form of the p.d.f. is known. Instead, you make approximations (re: educated guesses) as to this functional form through histograms.\n",
    "\n",
    "What is a histogram? Fundamentally, a histogram is a tool to bookkeep arrays of numbers:\n",
    "* binning\n",
    "* counts\n",
    "* uncertainties\n",
    "\n",
    "Beyond that, it contains helpful ingredients to make them more user-friendly for common operations (addition, division, etc...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the ingredients?\n",
    "\n",
    "Once you have a model, you can perform inference such as\n",
    "* exclusion fit (upper limits)\n",
    "* discovery fit (lower limits)\n",
    "* measurement (two-sided intervals)\n",
    "* parameter scans\n",
    "* impact plots\n",
    "* pull plots\n",
    "* ...\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/scikit-hep/pyhf/main/docs/_static/img/README_1bin_example.png\" alt=\"common operation - parameter scan\" width=400 />\n",
    "\n",
    "Let's make up some samples and histograms to go along with it to understand what's going on. Suppose we have an analysis with expected event rate $\\lambda$ and measurements $n$. For this simple case, the overall probability of the full experiment is the **joint probability** of each bin:\n",
    "\n",
    "$$\n",
    "p(n|\\lambda) = \\prod_{\\mathrm{bin}\\ b} \\mathrm{Pois}(n_b | \\lambda_b)\n",
    "$$\n",
    "\n",
    "A Poisson model is used as we are performing a counting experiment (counting the number of random events with an expected rate) in each bin of the observable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [1, 2, 3]\n",
    "observed = [3, 4, 4]\n",
    "expected_yields = [3.7, 3.2, 2.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(bins, expected_yields, 1.0, label=r\"Expected\", edgecolor=\"blue\", alpha=0.5)\n",
    "ax.scatter(bins, [3, 4, 4], color=\"black\", label=\"Observed\")\n",
    "ax.set_ylim(0, 6)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Observable\", fontsize=12)\n",
    "ax.set_ylabel(\"Count\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we don't always often have just a single expected (simulation) sample, and $\\lambda$ is often the sum of multiple sample yields\n",
    "\n",
    "$$\n",
    "\\lambda = \\sum_{\\mathrm{sample}\\ s} \\lambda_s\n",
    "$$\n",
    "\n",
    "A typical case might be multiple (sub)dominant backgrounds or having a model where the observed events are described by a signal + background p.d.f. The model is then\n",
    "\n",
    "$$\n",
    "p(n|\\lambda) = \\prod_{\\mathrm{bin}\\ b} \\mathrm{Pois}(n_b | \\lambda_b) \\qquad \\lambda_b = \\sum_{\\mathrm{sample}\\ s} \\lambda_{bs}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [1, 2, 3]\n",
    "observed = [3, 4, 4]\n",
    "background = [3.0, 1.5, 1.0]\n",
    "signal = [0.7, 1.7, 1.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(bins, background, 1.0, label=r\"Background\", edgecolor=\"red\", alpha=0.5)\n",
    "ax.bar(\n",
    "    bins, signal, 1.0, label=r\"Signal\", edgecolor=\"blue\", bottom=background, alpha=0.5\n",
    ")\n",
    "ax.scatter(bins, [3, 4, 4], color=\"black\", label=\"Observed\")\n",
    "ax.set_ylim(0, 6)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Observable\", fontsize=12)\n",
    "ax.set_ylabel(\"Count\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already, you can see the p.d.f. for this simple case starts expanding to be a little bit more generic, and a little bit more flexible. Now we want to incorporate when the expected yields for signal and backgrounds depend on some **parameters**, perhaps how we applied calibrations to some objects, or how we configured our Monte-Carlo generators, etc.\n",
    "\n",
    "Suppose we wanted a a normalization factor $\\mu_s$ scaling up (or down!) the sample. For example, if we want to parametrize the signal strength (without changing background). So $\\lambda$ becomes a function of $\\theta = \\{\\mu\\}$ (a set of the parameters that determine the expected event rate), then our p.d.f. expands to be\n",
    "\n",
    "$$\n",
    "p(n|\\lambda(\\mu)) = \\prod_{\\mathrm{bin}\\ b} \\mathrm{Pois}(n_b | \\lambda_b(\\theta)) \\qquad \\lambda_b(\\theta) = \\sum_{\\mathrm{sample}\\ s} \\lambda_{bs}(\\theta)\n",
    "$$\n",
    "\n",
    "where $\\mu_{\\mathrm{background}} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(mu=(0, 5, 0.1))\n",
    "def draw_plot(mu=1):\n",
    "    bins = [1, 2, 3]\n",
    "    observed = [3, 4, 4]\n",
    "    background = [3.0, 1.5, 1.0]\n",
    "    signal = [i * mu for i in [0.7, 1.7, 1.5]]\n",
    "\n",
    "    print(f\"signal: {signal}\")\n",
    "    print(f\"background: {background}\")\n",
    "    print(f\"observed: {observed}\\n\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(bins, background, 1.0, label=r\"Background\", edgecolor=\"red\", alpha=0.5)\n",
    "    ax.bar(\n",
    "        bins,\n",
    "        signal,\n",
    "        1.0,\n",
    "        label=r\"Signal\",\n",
    "        edgecolor=\"blue\",\n",
    "        bottom=background,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax.scatter(bins, [3, 4, 4], color=\"black\", label=\"Observed\")\n",
    "    ax.set_ylim(0, 6)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Observable\", fontsize=12)\n",
    "    ax.set_ylabel(\"Count\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final thing to finish our build up of a simplified HistFactory model is the concept of **auxiliary measurements**. Perhaps the background sample rate is modified by some normalization parameter, and we've made measurements of this parameter in a separate analysis (e.g. studies of the Jet Energy Scale). These prior experimental studies give a constraint that the parameter lies within a certain range.\n",
    "\n",
    "For some parameters in a statistical model we don't have prior experimental evidence for their values and must infer its values is the given analysis. These are **unconstrained** parameters ($\\eta$) and enter into the main model as parameters of the event rate $\\lambda(\\theta)$\n",
    "\n",
    "$$\n",
    "p(n | \\lambda(\\theta)).\n",
    "$$\n",
    "\n",
    "For many model parameters, their values in the model are constrained by a _constraint term function_, included in the model along with the the main model p.d.f, which describes **auxiliary measurements/data** ($a$) about the model parameter. These are **constrained** parameters ($\\chi$) and enter into the model both in the constraint terms and as parameters of the event rate $\\lambda(\\theta)$\n",
    "\n",
    "$$\n",
    "p_\\chi(a | \\chi)\n",
    "$$\n",
    "\n",
    "where $\\theta = \\{\\eta, \\chi\\}$. This constraining function model is chosen by the physics it represents, but in HistFactory most constraint terms are modeled as a Normal (Gaussian) or Poisson.\n",
    "\n",
    "With the constraint terms the model expands to be\n",
    "\n",
    "$$\n",
    "p(n,a|\\lambda(\\theta)) = \\prod_{\\mathrm{bin}\\ b} \\mathrm{Pois}(n_b | \\lambda_b(\\theta)) \\prod_{\\mathrm{constraint}\\ \\chi} p_\\chi(a_\\chi | \\chi) \\qquad \\lambda_b(\\theta) = \\sum_{\\mathrm{sample}\\ s} \\lambda_{bs}(\\theta)\n",
    "$$\n",
    "\n",
    "where the expected event rate $\\lambda_b(\\theta)$ is modified from its nominal value by a **chosen interpolation function** that smoothly interpolates between the up- and down-variations $(\\pm1 \\sigma)$ of the constraint term to provide an event rate modifier for any value of $\\chi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this simple example, let's consider a constraint term of a Normal distribution centered at $\\mu=0$ (\"auxiliary measurement\" $a=0$) with $\\sigma=1$ for constraining the normalization on the background where an up-variation ($\\mu_b = +1$) scales by 1.3, and a down-variation ($\\mu_b = -1$) scales by 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_constraint(mu_b=0.0):\n",
    "    # auxiliary measurement of 0\n",
    "    # though note that for Normal observation and mean are symmetric under exchange\n",
    "    return norm.pdf(0.0, loc=mu_b, scale=1.0)\n",
    "\n",
    "\n",
    "# selected interpolation function\n",
    "def interpolate(down, nom, up, alpha):\n",
    "    if alpha >= 0:\n",
    "        return (up - nom) * alpha + 1\n",
    "    else:\n",
    "        return 1 - (down - nom) * alpha\n",
    "\n",
    "\n",
    "@widgets.interact(mu=(0, 5, 0.1), mu_b=(-1, 1, 0.1))\n",
    "def draw_plot(mu=1, mu_b=0):\n",
    "    bins = [1, 2, 3]\n",
    "    observed = [3, 4, 4]\n",
    "    background = [i * interpolate(0.8, 1.0, 1.3, mu_b) for i in [3.0, 1.5, 1.0]]\n",
    "    signal = [i * mu for i in [0.7, 1.7, 1.5]]\n",
    "\n",
    "    print(f\"signal: {signal}\")\n",
    "    print(f\"background: {background}\")\n",
    "    print(f\"observed: {observed}\")\n",
    "    print(f\"likelihood scaled by: {normal_constraint(mu_b)/normal_constraint(0.0)}\\n\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(bins, background, 1.0, label=r\"Background\", edgecolor=\"red\", alpha=0.5)\n",
    "    ax.bar(\n",
    "        bins,\n",
    "        signal,\n",
    "        1.0,\n",
    "        label=r\"Signal\",\n",
    "        edgecolor=\"blue\",\n",
    "        bottom=background,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax.scatter(bins, [3, 4, 4], color=\"black\", label=\"Observed\")\n",
    "    ax.set_ylim(0, 6)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Observable\", fontsize=12)\n",
    "    ax.set_ylabel(\"Count\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, notice that all along, we've been only discussing a single \"channel\" with 3 bins. The statistical analysis being studied might involve **multiple channels** corresponding to different analysis signal regions and control regions. Therefore, we compute the likelihood as\n",
    "\n",
    "$$\n",
    "p_\\text{main} = p_\\text{channel1} * p_\\text{channel2} * p_\\text{channel3} \\cdots\n",
    "$$\n",
    "\n",
    "We then expand out the likelihood definition further across channels\n",
    "\n",
    "$$\n",
    "p(n,a|\\theta) = \\underbrace{\\prod_{\\mathrm{channel}\\ c}\\prod_{\\mathrm{bin}\\ b} \\mathrm{Pois}(n_{cb} | \\lambda_{cb}(\\theta))}_{\\text{main}}\\, \\underbrace{\\prod_{\\mathrm{constraint}\\ \\chi} p_\\chi(a_\\chi | \\chi)}_{\\text{auxiliary}} \\qquad \\lambda_{cb}(\\theta) = \\sum_{\\mathrm{sample}\\ s} \\lambda_{cbs}(\\theta)\n",
    "$$\n",
    "\n",
    "There are now two pieces of the model:\n",
    "* the main model, which consists of\n",
    "  * several channels (regions, histograms, etc), where\n",
    "  * each channel is a set of Poissons measuring the bin count for an expected value, where\n",
    "  * the expected value is the sum of various samples, where\n",
    "  * each samples expected value can be a function of parameters (or modifiers)\n",
    "* the constraint model, which consists of\n",
    "  * constraint terms on model parameters, where\n",
    "  * each constraint term describes auxiliary measurements\n",
    "  \n",
    "It should be clear by now that this is quite a lot of pieces to keep track of. This is where HistFactory comes in to play. Using HistFactory, we can\n",
    "* describe observed event rates and expected event rates\n",
    "* use well-defined **modifiers** to express parameterizations of the expected event rates\n",
    "* use well-defined **interpolation** mechanisms to derive expected event rates (if needed)\n",
    "* automatically handle auxiliary measurements / additional constraint terms\n",
    "\n",
    "*Note: if you're curious about interpolation and interesting challenges, see the next chapter.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyhf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up till 2018, HistFactory was only implemented using ROOT, RooStats, RooFit (+ minuit). pyhf provides two separate pieces:\n",
    "* a schema for serializing the HistFactory workspace in plain-text formats, such as JSON\n",
    "* a toolkit that interacts and manipulates the HistFactory workspaces\n",
    "\n",
    "Why is this crucial? HistFactory in ROOT is a combination of loosely-linked XML+ROOT files\n",
    "* XML for structure\n",
    "* ROOT for storing data\n",
    "\n",
    "These would then be processed through a `hist2workspace` command to get the ROOT Workspace that RooStats/RooFit use. As an example, let's look at the provided multichannel HistFactory XML+ROOT as part of this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lhR data/multichannel_histfactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have two folders:\n",
    "\n",
    "* `config` which provides\n",
    "  * the XML HistFactory schema [`HistFactorySchema.dtd`](./data/multichannel_histfactory/config/HistFactorySchema.dtd)\n",
    "  * a top-level [`example.xml`](./data/multichannel_histfactory/config/example.xml)\n",
    "  * signal region and control region structures\n",
    "* `data` which provides the stored histograms in [`data.root`](./data/multichannel_histfactory/data/data.root)\n",
    "\n",
    "Let's just look at the XML structure for now. What does the top-level look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat -n data/multichannel_histfactory/config/example.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This top-level specifies:\n",
    "* 15: the HistFactory XML schema\n",
    "* 17: the workspace definition\n",
    "* 18,19: channel definitions (links to other files)\n",
    "* 20: a measurement `GaussExample` with specifications for luminosity, the parameter of interest, and setting `lumi` constant\n",
    "\n",
    "What does the signal region look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat -n data/multichannel_histfactory/config/example_signal.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This signal region specifies:\n",
    "* 16: the HistFactory XML schema\n",
    "* 18: the channel definition along with the path to the file for where the data for this channel is stored\n",
    "* 19: where the expected event rate (data) for this channel is located\n",
    "* 20, 23: sample definitions for `signal` and `bkg` with each sample expected event rate stored under `HistoName` in the corresponding ROOT file\n",
    "* 21: a parameter `SigXsecOverSM` which is an unconstrained normalization factor\n",
    "* 24: a parameter `uncorrshape_signal` which is a Poisson-constrained shape systematic, with the corresponding auxiliary data stored under `HistoName` in the corresponding ROOT file\n",
    "\n",
    "As you can see, this works fine. It's a little bulky, and a lot of loosely-tied information, but this fulls specifies the HistFactory model we've discussed so far.\n",
    "\n",
    "In the next chapter, we'll learn how to use `pyhf` to convert to the HistFactory JSON representation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
