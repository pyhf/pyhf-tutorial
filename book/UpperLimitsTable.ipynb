{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "960720c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tarfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyhf\n",
    "import pyhf.contrib.utils\n",
    "import scipy\n",
    "from pyhf.contrib.viz import brazil\n",
    "\n",
    "pyhf.set_backend(\"numpy\", \"minuit\")\n",
    "\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "from rich.table import Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2335cc8",
   "metadata": {},
   "source": [
    "# Model-Independent Upper Limits\n",
    "\n",
    "This section is aiming to cover some common concepts about (model-independent) upper limits. While it won't necessarily be exhaustive, the aim is to summarize the current working knowledge of how High Energy Physics experiments such as ATLAS produces tables such as from the Supersymmetry Electroweak 2-lepton, 2-jet analysis ([SUSY-2018-05](https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SUSY-2018-05/)):\n",
    "\n",
    "<div style=\"text-align: center; padding: 25px 0;\">\n",
    "    <figure style=\"display: inline-grid; grid-template-columns: 1fr; grid-template-rows: auto auto;\">\n",
    "        <a href=\"https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SUSY-2018-05/\" target=\"_blank\"><img src=\"https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SUSY-2018-05/tab_20.png\" width=\"750\"/></a>\n",
    "        <figcaption style=\"width: 0; min-width: 100%;\">Table 20: Model-independent upper limits on the observed visible cross-section in the five electroweak search discovery regions, derived using pseudo-experiments. Left to right: background-only model post-fit total expected background, with the combined statistical and systematic uncertainties; observed data; 95 CL upper limits on the visible cross-section ($\\langle A\\varepsilon\\sigma\\rangle_\\mathrm{obs}^{95}$) and on the number of signal events ($\\mathrm{S}_\\mathrm{obs}^{95}$). The sixth column ($\\mathrm{S}_\\mathrm{exp}^{95}$) shows the expected 95 CL upper limit on the number of signal events, given the expected number (and $\\pm1\\sigma$ excursions of the expectation) of background events. The last two columns indicate the confidence level of the background-only hypothesis ($\\mathrm{CL}_\\mathrm{b}$) and discovery $p$-value with the corresponding Gaussian significance ($Z(s=0)$). $\\mathrm{CL}_\\mathrm{b}$ provides a measure of compatibility of the observed data with the signal strength hypothesis at the 95 CL limit relative to fluctuations of the background, and $p(s=0)$ measures compatibility of the observed data with the background-only hypothesis relative to fluctuations of the background. The $p$-value is capped at 0.5.\n",
    "        </figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "Here, the ATLAS collaboration provided some text describing each of the individual columns... but how does that translate to statistical fits and `pyhf`? Let's explore below. We will use the public probability models published on [HEPData entry](https://www.hepdata.net/record/116034) for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfb863f",
   "metadata": {},
   "source": [
    "## Downloading the public probability models\n",
    "\n",
    "First, we'll use `pyhf.contrib` to download the models from HEPData:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c411ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyhf.contrib.utils.download(\n",
    "    \"https://doi.org/10.17182/hepdata.116034.v1/r34\", \"workspace_2L2J\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9e136",
   "metadata": {},
   "source": [
    "Then use python's `tarfile` (part of standard library) to extract out the JSONs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acae3295",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(\"workspace_2L2J/ewk_discovery_bkgonly.json.tgz\") as fp:\n",
    "    bkgonly = pyhf.Workspace(json.load(fp.extractfile(\"ewk_discovery_bkgonly.json\")))\n",
    "\n",
    "with tarfile.open(\"workspace_2L2J/ewk_discovery_patchset.json.tgz\") as fp:\n",
    "    patchset = pyhf.PatchSet(json.load(fp.extractfile(\"ewk_discovery_patchset.json\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c9738f",
   "metadata": {},
   "source": [
    "and then build our workspace for a particular signal region **DR-Int-EWK**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfffecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = patchset.apply(bkgonly, \"DR_Int_EWK\")\n",
    "\n",
    "model = ws.model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92851f11",
   "metadata": {},
   "source": [
    "## Understanding expected and observed events\n",
    "\n",
    "The background estimate(s) in the signal region(s) are obtained from the background-only fit to control region(s) extrapolated to the signal region(s). \n",
    "\n",
    "This number, combined with the number of observed events in the signal region, allows us to redo the limit-setting or $p$-value determination without the public probability models.\n",
    "\n",
    "For the uncertainties on the background estimate(s) in the signal region(s), they're symmetric and calculated through standard error propagation described below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e284e2",
   "metadata": {},
   "source": [
    "### Observed Events\n",
    "\n",
    "The observed events are just `ws.data(model)` which we will call `data`. These are ordered in the same way as the ordering of the channels in the model (`model.config.channels`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ee1d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CRDY_cuts', 90.0), ('CRZZ_cuts', 194.0), ('CRZ_cuts', 159.0), ('CRtt_cuts', 424.0), ('DRInt_cuts', 38.0)]\n"
     ]
    }
   ],
   "source": [
    "data = ws.data(model)\n",
    "print(list(zip(model.config.channels, data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7068fd4",
   "metadata": {},
   "source": [
    "### Expected Events\n",
    "\n",
    "The expected events are determined from a background-only fit. This is done by fixing the parameter of interest (i.e. for SUSY the non-SM signal strength) to zero. This fit will give us the fitted parameters $\\hat{\\theta}$. From this, we can print out the expected background events in each channel after this fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f79763d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">  Per-region yields from   </span>\n",
       "<span style=\"font-style: italic\">    background-only fit    </span>\n",
       " ───────────────────────── \n",
       " <span style=\"font-weight: bold\"> Region     </span> <span style=\"font-weight: bold\"> Total Bkg. </span> \n",
       " ───────────────────────── \n",
       " <span style=\"color: #008080; text-decoration-color: #008080\"> CRDY_cuts  </span>    90.03     \n",
       " <span style=\"color: #008080; text-decoration-color: #008080\"> CRZZ_cuts  </span>    196.13    \n",
       " <span style=\"color: #008080; text-decoration-color: #008080\"> CRZ_cuts   </span>    159.00    \n",
       " <span style=\"color: #008080; text-decoration-color: #008080\"> CRtt_cuts  </span>    424.33    \n",
       " <span style=\"color: #008080; text-decoration-color: #008080\"> DRInt_cuts </span>    35.54     \n",
       " ───────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m  Per-region yields from   \u001b[0m\n",
       "\u001b[3m    background-only fit    \u001b[0m\n",
       " ───────────────────────── \n",
       " \u001b[1m \u001b[0m\u001b[1mRegion    \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mTotal Bkg.\u001b[0m\u001b[1m \u001b[0m \n",
       " ───────────────────────── \n",
       " \u001b[36m \u001b[0m\u001b[36mCRDY_cuts \u001b[0m\u001b[36m \u001b[0m    90.03     \n",
       " \u001b[36m \u001b[0m\u001b[36mCRZZ_cuts \u001b[0m\u001b[36m \u001b[0m    196.13    \n",
       " \u001b[36m \u001b[0m\u001b[36mCRZ_cuts  \u001b[0m\u001b[36m \u001b[0m    159.00    \n",
       " \u001b[36m \u001b[0m\u001b[36mCRtt_cuts \u001b[0m\u001b[36m \u001b[0m    424.33    \n",
       " \u001b[36m \u001b[0m\u001b[36mDRInt_cuts\u001b[0m\u001b[36m \u001b[0m    35.54     \n",
       " ───────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pars_uncrt_bonly, corr_bonly = pyhf.infer.mle.fixed_poi_fit(\n",
    "    0.0, data, model, return_uncertainties=True, return_correlations=True\n",
    ")\n",
    "pars_bonly, uncrt_bonly = pars_uncrt_bonly.T\n",
    "\n",
    "table = Table(title=\"Per-region yields from background-only fit\", box=box.HORIZONTALS)\n",
    "table.add_column(\"Region\", justify=\"left\", style=\"cyan\", no_wrap=True)\n",
    "table.add_column(\"Total Bkg.\", justify=\"center\")\n",
    "\n",
    "for region, count in zip(model.config.channels, model.expected_actualdata(pars_bonly)):\n",
    "    table.add_row(region, f\"{count:0.2f}\")\n",
    "\n",
    "console = Console()\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3940313d",
   "metadata": {},
   "source": [
    "The uncertainty on the total background (expected event rate) is done by *linearly* propagating errors on parameters using the covariance matrix from a fit result. The error is calculated in the same way that [`RooAbsReal::getPropagatedError()`](https://root.cern.ch/doc/master/classRooAbsReal.html#a4d7678837410aabcd93e6f0937de525b) calculates it. It is summarized as follows\n",
    "\n",
    "$$\n",
    "\\mathrm{error}^2(x) = F_\\mathbf{a}(x) \\cdot \\mathrm{Corr}(\\mathbf{a},\\mathbf{a}') \\cdot F_{\\mathbf{a}'}^{\\mathrm{T}}(x)\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "F_\\mathbf{a}(x) = \\frac{ f(x, \\mathbf{a} + \\mathrm{d}\\mathbf{a}) - f(x, \\mathbf{a} - \\mathrm{d}\\mathbf{a}) }{2},\n",
    "$$\n",
    "\n",
    "with $f(x)$ the probability model and $\\mathrm{d}\\mathbf{a}$ the vector of one-sigma uncertainties of all fit parameters taken from the fit result and $\\mathrm{Corr}(\\mathbf{a},\\mathbf{a}')$ the correlation matrix from the fit result.\n",
    "\n",
    "References:\n",
    "- [Statistical Error Propagation (J. Phys. Chem. A 2001, 105, 15, 3917–3921)](https://web.stanford.edu/~kimth/www-mit/8.13/error_propagation.pdf)\n",
    "- [root-project/root#13404](https://github.com/root-project/root/issues/13404)\n",
    "\n",
    "**Note**: be aware that some definitions of error propagation use covariance and some use correlation. You can translate between the two, such turning covariance into correlation by multiplying by $\\partial a$ on both sides. If one uses covariance, you'll need information about the derivatives (e.g. through auto-differentiation).\n",
    "\n",
    "This could be done with a for-loop like so:\n",
    "\n",
    "```python\n",
    "up_yields = []\n",
    "dn_yields = []\n",
    "for i, variation in enumerate(uncrt_bonly):\n",
    "    varied_up_pars = pars_bonly[:] # copy\n",
    "    varied_up_pars[i] += variation\n",
    "    \n",
    "    varied_dn_pars = pars_bonly[:] # copy\n",
    "    varied_dn_pars[i] -= variation\n",
    "\n",
    "    up_yields.append(model.expected_actualdata(varied_up_pars))\n",
    "    dn_yields.append(model.expected_actualdata(varied_dn_pars))\n",
    "```\n",
    "\n",
    "which gives us the up/down variations on the yields for each systematic. However, we'll batch this calculation to calculate all yield variations simultaneously. To do this, we set up our model to allow for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34c480b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "npars = len(model.config.parameters)\n",
    "model_batch = ws.model(batch_size=npars)\n",
    "pars_batch = np.tile(pars_bonly, (npars, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6a42f",
   "metadata": {},
   "source": [
    "And then we go ahead and calculate the `up_yields` and `dn_yields` which is used to calculate the $F_\\mathbf{a}(x)$ term above, denoted as `variations`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a9f6848",
   "metadata": {},
   "outputs": [],
   "source": [
    "up_yields = model_batch.expected_actualdata(pars_batch + np.diag(uncrt_bonly))\n",
    "dn_yields = model_batch.expected_actualdata(pars_batch - np.diag(uncrt_bonly))\n",
    "variations = (up_yields - dn_yields) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd1b3b9",
   "metadata": {},
   "source": [
    "Now we can calculate $\\mathrm{error}^2(x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcc4aa16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 89.50743792, 188.1423674 , 158.73206077, 423.84859688,\n",
       "        16.934086  ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_sq = np.einsum(\"il,ik,kl->l\", variations, corr_bonly, variations)\n",
    "error_sq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d3365",
   "metadata": {},
   "source": [
    "And of course, taking the square root will give us the propagated uncertainty on the yields per-region for the background-only fit. We can update our table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c3023bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">    Per-region yields from     </span>\n",
       "<span style=\"font-style: italic\">      background-only fit      </span>\n",
       " ───────────────────────────── \n",
       " <span style=\"font-weight: bold\"> Region     </span> <span style=\"font-weight: bold\">   Total Bkg.   </span> \n",
       " ───────────────────────────── \n",
       " <span style=\"color: #008080; text-decoration-color: #008080\"> CRDY_cuts  </span>   90.03 ± 9.46   \n",
       " <span style=\"color: #008080; text-decoration-color: #008080\"> CRZZ_cuts  </span>  196.13 ± 13.72  \n",
       " <span style=\"color: #008080; text-decoration-color: #008080\"> CRZ_cuts   </span>  159.00 ± 12.60  \n",
       " <span style=\"color: #008080; text-decoration-color: #008080\"> CRtt_cuts  </span>  424.33 ± 20.59  \n",
       " <span style=\"color: #008080; text-decoration-color: #008080\"> DRInt_cuts </span>   35.54 ± 4.12   \n",
       " ───────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m    Per-region yields from     \u001b[0m\n",
       "\u001b[3m      background-only fit      \u001b[0m\n",
       " ───────────────────────────── \n",
       " \u001b[1m \u001b[0m\u001b[1mRegion    \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m  Total Bkg.  \u001b[0m\u001b[1m \u001b[0m \n",
       " ───────────────────────────── \n",
       " \u001b[36m \u001b[0m\u001b[36mCRDY_cuts \u001b[0m\u001b[36m \u001b[0m   90.03 ± 9.46   \n",
       " \u001b[36m \u001b[0m\u001b[36mCRZZ_cuts \u001b[0m\u001b[36m \u001b[0m  196.13 ± 13.72  \n",
       " \u001b[36m \u001b[0m\u001b[36mCRZ_cuts  \u001b[0m\u001b[36m \u001b[0m  159.00 ± 12.60  \n",
       " \u001b[36m \u001b[0m\u001b[36mCRtt_cuts \u001b[0m\u001b[36m \u001b[0m  424.33 ± 20.59  \n",
       " \u001b[36m \u001b[0m\u001b[36mDRInt_cuts\u001b[0m\u001b[36m \u001b[0m   35.54 ± 4.12   \n",
       " ───────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = Table(title=\"Per-region yields from background-only fit\", box=box.HORIZONTALS)\n",
    "table.add_column(\"Region\", justify=\"left\", style=\"cyan\", no_wrap=True)\n",
    "table.add_column(\"Total Bkg.\", justify=\"center\")\n",
    "\n",
    "for region, count, uncrt in zip(\n",
    "    model.config.channels, model.expected_actualdata(pars_bonly), np.sqrt(error_sq)\n",
    "):\n",
    "    table.add_row(region, f\"{count:0.2f} ± {uncrt:0.2f}\")\n",
    "\n",
    "console = Console()\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8123d0f3",
   "metadata": {},
   "source": [
    "## Understanding visible cross-section\n",
    "\n",
    "_See [ATL-PHYS-PUB-2022-017](https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PUBNOTES/ATL-PHYS-PUB-2022-017/) for descriptions of detector efficiency and selection acceptance._\n",
    "\n",
    "**Production cross section upper limits** is another way of saying model-dependent limits. Without the detector efficiency (but keeping selection acceptance), the term **Visible cross section upper limits** is used. \n",
    "\n",
    "For upper limits on numbers of events (and the corresponding cross section), after detector efficiency and selection acceptance effects, the term **Model independent upper limits** is used. These upper limits are typically free from assumptions about the acceptance and efficiency of any model (and are model independent in that sense), but they do make the assumption that signal contamination in control regions is negligible.\n",
    "\n",
    "### Model-independent Upper Limits\n",
    "\n",
    "Model-independent upper limits at 95% CL on the number of beyond the SM (BSM) events for each signal region are derived using the $\\mathrm{CL}_\\mathrm{s}$ prescription [10.1088/0954-3899/28/10/313](https://doi.org/10.1088/0954-3899/28/10/313) and neglecting any possible contamination in the control regions.\n",
    "\n",
    "To obtain the 95% model-independent UL / discovery $p_0$-value, the fit in the signal region proceeds in the same way as the background-only fit, except that an additional parameter for the non-SM signal strength, constrained to be non-negative, is fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57fda65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyhf.set_backend(\"numpy\", \"scipy\")\n",
    "\n",
    "mu_tests = np.linspace(15, 25, 10)\n",
    "(\n",
    "    obs_limit,\n",
    "    exp_limit_and_bands,\n",
    "    (poi_tests, tests),\n",
    ") = pyhf.infer.intervals.upper_limits.upper_limit(\n",
    "    data, model, scan=mu_tests, return_results=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4190909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed UL: 20.15\n",
      "Expected UL: 17.00 [-2.00, +6.33]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Observed UL: {obs_limit:0.2f}\")\n",
    "print(\n",
    "    f\"Expected UL: {exp_limit_and_bands[2]:0.2f} [{exp_limit_and_bands[1]-exp_limit_and_bands[2]:0.2f}, +{exp_limit_and_bands[3]-exp_limit_and_bands[2]:0.2f}]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c5e954",
   "metadata": {},
   "source": [
    "When calculating $S^{95}_\\mathrm{obs}$ (also for expected) -- it is typical to inject a _dummy_ signal (all expected event rates equal to unity) which means for this signal, we're assuming $A\\times\\varepsilon\\times\\sigma\\times\\mathcal{L}=1$. As $S^{95}_\\mathrm{obs}$ is an upper limit on $\\mu$ for this _dummy_ signal, normalizing this by the integrated luminosity of the data sample allows us to interpret as upper limits on the **visible (BSM) cross-section**, $\\langle A\\varepsilon\\sigma\\rangle_\\mathrm{obs}^{95}$. This is defined as the product of acceptance, reconstruction efficiency and production cross-section.\n",
    "\n",
    "Remember that we often call this the **visible cross-section**, where cross-section is $\\sigma$ and the visible cross-section is $A\\times\\varepsilon\\times\\sigma$. This is called a _model independent_ limit as we do not have a specific model in mind (hence the _dummy_ signal). We don't have specific values for selection acceptance $A$, detector efficiency $\\varepsilon$, or signal model cross-section $\\sigma$, so we can only say something about the product. If we have a specific signal model with $A\\varepsilon\\sigma$ that gives us a cross-section higher than the limit on the visible cross-section, we should be able to see that signal in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f5b5e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit on visible cross-section: 0.14\n"
     ]
    }
   ],
   "source": [
    "print(f\"Limit on visible cross-section: {obs_limit / 140:0.2f}\")  # 140 ifb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e055f6",
   "metadata": {},
   "source": [
    "## Discovery p-values\n",
    "\n",
    "The significance of an excess can be quantified by the probability ($p_0$) that a background-only experiment is more signal-like than observed. The last two columns indicate the $\\mathrm{CL}_\\mathrm{b}$ value and the discovery p-value ($p(s)=0$) with the corresponding gaussian significance ($Z$).\n",
    "\n",
    "These two numbers are giving you information about the compatibility of the observed data with a background-only hypothesis, albeit with different test statistics $q$. The $q_\\mu$ (or $\\tilde{q}_\\mu$) test statistic is optimized for hypothesis-testing the signal+background hypothesis. The $q_0$ test statistic is optimized for hypothesis-testing the background-only hypothesis.\n",
    "\n",
    "From equation 52 in [arXiv:1503.07622](https://arxiv.org/pdf/1503.07622.pdf):\n",
    "- for $p_\\mu$: the null hypothesis is signal at a rate $\\mu' < \\mu$ and the alternative hypothesis is signal at a rate of $\\mu$\n",
    "- for $p_0$: the null hypothesis is background-only (b-only) and alternative hypothesis is signal+background (s+b)\n",
    "\n",
    "For these hypotheses, we find that the $p$-values are\n",
    "\n",
    "$$\n",
    "1 - p_\\mathrm{b} = P(q \\geq q_\\mathrm{obs} | \\mathrm{b-only}) = \\int_{q_\\mathrm{obs}}^\\infty f(q | \\mathrm{b-only} )\\ \\mathrm{d}q\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "p_\\mathrm{s+b} = P(q \\geq q_\\mathrm{obs} | \\mathrm{s+b}) = \\int_{q_\\mathrm{obs}}^\\infty f(q | \\mathrm{s+b} )\\ \\mathrm{d}q\n",
    "$$\n",
    "\n",
    "Note that the same $q_\\mathrm{obs}$ value is used to determine $p_\\mu$ and $p_b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b092d468",
   "metadata": {},
   "source": [
    "### $p_\\mu$ (and $\\mathrm{CL}_\\mathrm{b}$)\n",
    "\n",
    "$\\mathrm{CL}_\\mathrm{b}$ provides a measure of compatibility of the observed data with the 95% CL signal strength hypothesis relative to fluctuations of the background. $\\mathrm{CL}_\\mathrm{b}$ is evaluating $p_\\mu$ in the case where $\\mu'=0$ and $\\mu=\\mu_{95}$ (the fitted $\\mu$ at the upper limit $S^{95}_\\mathrm{obs}$). This is saying that the tested $\\mu$ is the 95% CL signal strength ($\\mu_{95}$) but the assumed true $\\mu$ is the background-only hypothesis ($\\mu'=0$):\n",
    "\n",
    "$$\n",
    "\\mathrm{CL}_\\mathrm{b} \\equiv 1 - p_b = \\int_{q_{\\mu_{95}}^{\\ \\mathrm{obs}}}^\\infty f(q_\\mu | \\mu' = 0)\\ \\mathrm{d} q_\\mu\n",
    "$$\n",
    "\n",
    "Note that this calculation depends on the value of $\\mu$ at the fitted upper limit $S^{95}_\\mathrm{obs}$. You get a small $\\mathrm{CL}_\\mathrm{b}$ if you have a downward fluctuation in observed data relative to your signal+background hypothesis, preventing you from excluding your signal erroneously if you \"shouldn't be able to\" when using $\\mathrm{CL}_\\mathrm{s}$ (since you would be dividing by a small number). See the paragraph above plot on page two in [this document](https://twiki.cern.ch/twiki/pub/Sandbox/NotesOnStatistics/CLsInfo.pdf).\n",
    "\n",
    "### $p_0$\n",
    "\n",
    "$p(s)=0$ measures compatibility of the observed data with the background-only (zero signal strength) hypothesis relative to fluctuations of the background. Larger values indicate greater relative compatibility. $p(s)=0$ is not calculated in signal regions with a deficit with respect to the nominal background prediction.\n",
    "\n",
    "This is saying that the tested $\\mu$ is the background-only hypothesis ($\\mu=\\mu'=0$) and the assumed true $\\mu$ is the background-only hypothesis:\n",
    "\n",
    "$$\n",
    "p_0 = P(q_0 \\geq q_0^\\mathrm{obs}) = \\int_{q_{\\mu_{95}}^{\\ \\mathrm{obs}}}^\\infty f(q_{\\mu=0} | \\mu'=0)\\ \\mathrm{d} q_{\\mu=0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa48732",
   "metadata": {},
   "source": [
    "### Relationship to $\\mathrm{CL}_\\mathrm{s}$\n",
    "\n",
    "Alex Read also provides two slides that illustrates the relationship between these two quantities. $1-p_b$ and $p_\\mu$ must be evaluated with the same value of $\\mu$ in order to implement the properties of $\\mathrm{CL}_\\mathrm{s}$.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <figure style=\"display: flex; grid-template-columns: 1fr 1fr;\">\n",
    "        <div>\n",
    "            <img src=\"./PastedGraphic-1.png\" width=\"100%\"/>\n",
    "        </div>\n",
    "        <div>\n",
    "            <img src=\"./PastedGraphic-2.png\" width=\"100%\"/>\n",
    "        </div>\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005b6d1c",
   "metadata": {},
   "source": [
    "### Calculating the $p$-values\n",
    "\n",
    "Armed with all of this knowledge, we can first calculate the discovery $p$-value evaluated at $\\mu=\\mu'=0$, as well as convert this to a Z-score assuming a \"standard\" Gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11011192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(s)=0 (Z): 0.24 (0.72)\n"
     ]
    }
   ],
   "source": [
    "p0 = pyhf.infer.hypotest(\n",
    "    0.0, data, model, test_stat=\"q0\"\n",
    ")  # , calctype='toybased', ntoys=10000)\n",
    "p0_Z = scipy.stats.norm.isf(p0)\n",
    "print(f\"p(s)=0 (Z): {p0:0.2f} ({p0_Z:0.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f10a3ca",
   "metadata": {},
   "source": [
    "And similarly, we can extract $\\mathrm{CL}_\\mathrm{b}$ by evaluating at $\\mu=\\mu_{95}^\\mathrm{obs}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74072887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLb: 0.70\n"
     ]
    }
   ],
   "source": [
    "_, (_, CLb) = pyhf.infer.hypotest(obs_limit, data, model, return_tail_probs=True)\n",
    "print(f\"CLb: {CLb:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2586d091",
   "metadata": {},
   "source": [
    "## Making the table\n",
    "\n",
    "Now that we have all of our numbers, we can go ahead and make the table for the discovery region `DRInt_cuts`. We'll just get the corresponding index of the channel so we can extract out the total background and uncertainty estimates from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84be7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_mask = model.config.channels.index(\"DRInt_cuts\")\n",
    "bkg_count = model.expected_actualdata(pars_bonly)[region_mask]\n",
    "bkg_uncrt = np.sqrt(error_sq)[region_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d7ee562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">Model-independent upper limits on the observed visible cross-section in the five electroweak search</span>\n",
       "<span style=\"font-style: italic\">          discovery regions; derived using asymptotics, pyhf, and published likelihoods.           </span>\n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────── \n",
       " <span style=\"font-weight: bold\"> Signal Region </span> <span style=\"font-weight: bold\"> Total Bkg. </span> <span style=\"font-weight: bold\"> Data </span> <span style=\"font-weight: bold\"> XSec </span> <span style=\"font-weight: bold\"> S95Obs </span> <span style=\"font-weight: bold\"> S95Exp           </span> <span style=\"font-weight: bold\"> CLb(S95Obs) </span> <span style=\"font-weight: bold\"> p(s=0) (Z) </span> \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────── \n",
       " <span style=\"color: #008080; text-decoration-color: #008080\"> DRInt_cuts    </span>    36 ± 4     38     0.14   20.2     17.0 + 6.3 - 2.0   0.70          0.24 (0.7)  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3mModel-independent upper limits on the observed visible cross-section in the five electroweak search\u001b[0m\n",
       "\u001b[3m          discovery regions; derived using asymptotics, pyhf, and published likelihoods.           \u001b[0m\n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────── \n",
       " \u001b[1m \u001b[0m\u001b[1mSignal Region\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mTotal Bkg.\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mData\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mXSec\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mS95Obs\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mS95Exp          \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mCLb(S95Obs)\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mp(s=0) (Z)\u001b[0m\u001b[1m \u001b[0m \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────── \n",
       " \u001b[36m \u001b[0m\u001b[36mDRInt_cuts   \u001b[0m\u001b[36m \u001b[0m    36 ± 4     38     0.14   20.2     17.0 + 6.3 - 2.0   0.70          0.24 (0.7)  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = Table(\n",
    "    title=\"Model-independent upper limits on the observed visible cross-section in the five electroweak search discovery regions; derived using asymptotics, pyhf, and published likelihoods.\",\n",
    "    box=box.HORIZONTALS,\n",
    ")\n",
    "table.add_column(\"Signal Region\", justify=\"left\", style=\"cyan\", no_wrap=True)\n",
    "table.add_column(\"Total Bkg.\", justify=\"center\")\n",
    "table.add_column(\"Data\")\n",
    "table.add_column(\"XSec\")\n",
    "table.add_column(\"S95Obs\")\n",
    "table.add_column(\"S95Exp\")\n",
    "table.add_column(\"CLb(S95Obs)\")\n",
    "table.add_column(\"p(s=0) (Z)\")\n",
    "\n",
    "table.add_row(\n",
    "    model.config.channels[region_mask],\n",
    "    f\"{bkg_count:0.0f} ± {bkg_uncrt:0.0f}\",\n",
    "    f\"{data[region_mask]:0.0f}\",\n",
    "    f\"{obs_limit / 140:0.2f}\",\n",
    "    f\"{obs_limit:0.1f}\",\n",
    "    f\"{exp_limit_and_bands[2]:0.1f} + {exp_limit_and_bands[3] - exp_limit_and_bands[2]:0.1f} - {exp_limit_and_bands[2] - exp_limit_and_bands[1]:0.1f}\",\n",
    "    f\"{CLb:0.2f}\",\n",
    "    f\"{p0:0.2f} ({p0_Z:0.1f})\",\n",
    ")\n",
    "\n",
    "console = Console()\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334983cb",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; padding: 25px 0;\">\n",
    "    <figure style=\"display: inline-grid; grid-template-columns: 1fr; grid-template-rows: auto auto;\">\n",
    "        <a href=\"https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SUSY-2018-05/\" target=\"_blank\"><img src=\"https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SUSY-2018-05/tab_20.png\" width=\"750\"/></a>\n",
    "        <figcaption style=\"width: 0; min-width: 100%;\">Table 20: Model-independent upper limits on the observed visible cross-section in the five electroweak search discovery regions, derived using pseudo-experiments. Left to right: background-only model post-fit total expected background, with the combined statistical and systematic uncertainties; observed data; 95 CL upper limits on the visible cross-section ($\\langle A\\varepsilon\\sigma\\rangle_\\mathrm{obs}^{95}$) and on the number of signal events ($\\mathrm{S}_\\mathrm{obs}^{95}$). The sixth column ($\\mathrm{S}_\\mathrm{exp}^{95}$) shows the expected 95 CL upper limit on the number of signal events, given the expected number (and $\\pm1\\sigma$ excursions of the expectation) of background events. The last two columns indicate the confidence level of the background-only hypothesis ($\\mathrm{CL}_\\mathrm{b}$) and discovery $p$-value with the corresponding Gaussian significance ($Z(s=0)$). $\\mathrm{CL}_\\mathrm{b}$ provides a measure of compatibility of the observed data with the signal strength hypothesis at the 95 CL limit relative to fluctuations of the background, and $p(s=0)$ measures compatibility of the observed data with the background-only hypothesis relative to fluctuations of the background. The $p$-value is capped at 0.5.\n",
    "        </figcaption>\n",
    "    </figure>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca56370",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "We acknowledge the help of ATLAS Statistics Forum including (alphabetical order):\n",
    " - Will Buttinger\n",
    " - Jonathan Long\n",
    " - Zach Marshall\n",
    " - Alex Read\n",
    " - Sarah Williams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
