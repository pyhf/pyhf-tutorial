{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with Toys\n",
    "\n",
    "As of `v0.6.0`, `pyhf` now supports toys! A lot of kinks have been discovered and worked out and we're grateful to our ATLAS colleagues for beta-testing this in the meantime. We don't believe that there may not be any more bugs, but we feel confident that we can release the current implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyhf\n",
    "\n",
    "model = pyhf.simplemodels.uncorrelated_background(\n",
    "    signal=[5.0, 10.0], bkg=[50.0, 60.0], bkg_uncertainty=[5.0, 12.0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing (revisited)\n",
    "\n",
    "And like in the first exercise we did, let's refresh what the hypothesis test looked like using $\\tilde{q}_\\mu$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "    1.0,  # null hypothesis\n",
    "    [53.0, 65.0] + model.config.auxdata,\n",
    "    model,\n",
    "    test_stat=\"qtilde\",\n",
    "    return_expected_set=True,\n",
    ")\n",
    "print(f\"      Observed CLs: {CLs_obs:.4f}\")\n",
    "for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "    print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the question is, does the asymptotic approximation hold in this example? The standard assumption is that you have enough \"statistics\" (meaning enough events) to use the large-N approximation. So let's use the toy-based calculator instead and compute the same values as above and see if they match in the asymptotic case (we certainly hope they mostly do here!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "    1.0,  # null hypothesis\n",
    "    [53.0, 65.0] + model.config.auxdata,\n",
    "    model,\n",
    "    test_stat=\"qtilde\",\n",
    "    return_expected_set=True,\n",
    "    calctype=\"toybased\",\n",
    "    ntoys=1000,\n",
    "    track_progress=False,\n",
    ")\n",
    "print(f\"      Observed CLs: {CLs_obs:.4f}\")\n",
    "for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "    print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that this time, a progress bar pops up! This is running the fits for each of these toys. We're hard at work to find new ways to improve the performance of this, but evaluating 50+ toys/s is not very bad for the initial implementation.\n",
    "\n",
    "Overall, this is not so bad given that we're only running 1000 toys. What does this look like in a case where we have lower statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing (low stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_low = pyhf.simplemodels.uncorrelated_background(\n",
    "    signal=[0.5, 1.0], bkg=[5.0, 6.0], bkg_uncertainty=[0.5, 1.2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the asymptotics case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "    1.0,  # null hypothesis\n",
    "    [5.0, 7.0] + model.config.auxdata,\n",
    "    model,\n",
    "    test_stat=\"qtilde\",\n",
    "    return_expected_set=True,\n",
    "    calctype=\"asymptotics\",\n",
    ")\n",
    "print(f\"      Observed CLs: {CLs_obs:.4f}\")\n",
    "for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "    print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now throwing 2000 toys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "    1.0,  # null hypothesis\n",
    "    [5.0, 7.0] + model.config.auxdata,\n",
    "    model,\n",
    "    test_stat=\"qtilde\",\n",
    "    return_expected_set=True,\n",
    "    calctype=\"toybased\",\n",
    "    ntoys=1000,\n",
    "    track_progress=False,\n",
    ")\n",
    "print(f\"      Observed CLs: {CLs_obs:.4f}\")\n",
    "for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "    print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as you can see in the case of lower statistics, the asymptotic approximation starts failing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
